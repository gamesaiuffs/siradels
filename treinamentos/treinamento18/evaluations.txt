------------------------------
treinamento\1
Rodadas: 100 Steps: 20000 Vit�rias: 19 M�dia de pontos 15.21
------------------------------
treinamento\2
Rodadas: 100 Steps: 40000 Vit�rias: 19 M�dia de pontos 14.46
------------------------------
treinamento\3
Rodadas: 100 Steps: 60000 Vit�rias: 17 M�dia de pontos 13.85
------------------------------
treinamento\4
Rodadas: 100 Steps: 80000 Vit�rias: 13 M�dia de pontos 12.78
------------------------------
treinamento\5
Rodadas: 100 Steps: 100000 Vit�rias: 15 M�dia de pontos 13.81
------------------------------
treinamento\6
Rodadas: 100 Steps: 120000 Vit�rias: 24 M�dia de pontos 14.8
------------------------------
treinamento\7
Rodadas: 100 Steps: 140000 Vit�rias: 20 M�dia de pontos 13.38
------------------------------
treinamento\8
Rodadas: 100 Steps: 160000 Vit�rias: 22 M�dia de pontos 14.14
------------------------------
treinamento\9
Rodadas: 100 Steps: 180000 Vit�rias: 17 M�dia de pontos 14.11
------------------------------
treinamento\10
Rodadas: 100 Steps: 200000 Vit�rias: 21 M�dia de pontos 15.47
------------------------------
treinamento\11
Rodadas: 100 Steps: 220000 Vit�rias: 16 M�dia de pontos 14.25
------------------------------
treinamento\12
Rodadas: 100 Steps: 240000 Vit�rias: 21 M�dia de pontos 14.01
------------------------------
treinamento\13
Rodadas: 100 Steps: 260000 Vit�rias: 12 M�dia de pontos 12.65
------------------------------
treinamento\14
Rodadas: 100 Steps: 280000 Vit�rias: 18 M�dia de pontos 13.65
------------------------------
treinamento\15
Rodadas: 100 Steps: 300000 Vit�rias: 15 M�dia de pontos 13.5

    model = DQN(
            "MlpPolicy",                     # Política de rede neural MLP
            env=env,                         # Ambiente de OpenAI Gym
            verbose=3,                       # Nível de detalhamento dos logs
            tau=0.7,   
            # Parâmetros de exploração
            exploration_initial_eps=0.8,     # Taxa inicial de exploração alta
            exploration_final_eps=0.05,       # Taxa final de exploração baixa
            exploration_fraction=0.2,        # Fração do total de etapas dedicadas à exploração

            # Parâmetros de treinamento e otimização
            learning_rate= 1e-4,            # Taxa de aprendizado
            learning_starts=5000,            # Número de etapas de aprendizado antes de começar a treinar
            gradient_steps=1,               # Número de passos de gradiente (padrão usa -1, que é automático)
            policy_kwargs=dict(net_arch=[32]),  # Arquitetura da rede neural

            # Parâmetros de desconto e frequência de treinamento
            gamma=0.95,                      # Fator de desconto
            train_freq=1000,                    # Frequência de treinamento 

            # Parâmetros do replay buffer
            buffer_size=100000,               # Tamanho do buffer de replay
            batch_size=256,                  # Tamanho do lote de amostras para o treinamento
            target_update_interval=8000,     # Intervalo de atualização do alvo
    )