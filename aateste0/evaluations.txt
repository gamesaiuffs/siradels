------------------------------
aateste0\1
Rodadas: 100 Steps: 10000 Vit�rias: 18 M�dia de pontos 14.16
------------------------------
aateste0\2
Rodadas: 100 Steps: 20000 Vit�rias: 17 M�dia de pontos 13.75
------------------------------
aateste0\3
Rodadas: 100 Steps: 30000 Vit�rias: 22 M�dia de pontos 14.59
------------------------------
aateste0\4
Rodadas: 100 Steps: 40000 Vit�rias: 23 M�dia de pontos 13.87
------------------------------
aateste0\5
Rodadas: 100 Steps: 50000 Vit�rias: 15 M�dia de pontos 13.67
------------------------------
aateste0\6
Rodadas: 100 Steps: 60000 Vit�rias: 16 M�dia de pontos 13.66
------------------------------
aateste0\7
Rodadas: 100 Steps: 70000 Vit�rias: 20 M�dia de pontos 14.69
------------------------------
aateste0\8
Rodadas: 100 Steps: 80000 Vit�rias: 11 M�dia de pontos 13.32
------------------------------
aateste0\9
Rodadas: 100 Steps: 90000 Vit�rias: 21 M�dia de pontos 14.63
------------------------------
aateste0\10
Rodadas: 100 Steps: 100000 Vit�rias: 18 M�dia de pontos 13.68


 # model = DQN(
    #         "MlpPolicy",                     # Política de rede neural MLP
    #         env=env,                         # Ambiente de OpenAI Gym
    #         verbose=3,                       # Nível de detalhamento dos logs
    #         tau=0.7,   
    #         # Parâmetros de exploração
    #         exploration_initial_eps=0.8,     # Taxa inicial de exploração alta
    #         exploration_final_eps=0.05,       # Taxa final de exploração baixa
    #         exploration_fraction=0.5,        # Fração do total de etapas dedicadas à exploração

    #         # Parâmetros de treinamento e otimização
    #         learning_rate= 1e-4,            # Taxa de aprendizado
    #         learning_starts=5000,            # Número de etapas de aprendizado antes de começar a treinar
    #         gradient_steps=1,               # Número de passos de gradiente (padrão usa -1, que é automático)
    #         policy_kwargs=dict(net_arch=[32]),  # Arquitetura da rede neural

    #         # Parâmetros de desconto e frequência de treinamento
    #         gamma=0.95,                      # Fator de desconto
    #         train_freq=1000,                    # Frequência de treinamento 

    #         # Parâmetros do replay buffer
    #         buffer_size=100000,               # Tamanho do buffer de replay
    #         batch_size=256,                  # Tamanho do lote de amostras para o treinamento
    #         target_update_interval=8000,     # Intervalo de atualização do alvo
    # )